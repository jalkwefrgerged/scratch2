I help risk and compliance teams turn complex regulatory expectations into clear, defensible outcomes that hold up under review. I translate requirements into a deliverable-driven plan, align stakeholders on decisions and ownership, and drive the underlying analysis and writing to an audit-traceable standard. When it adds leverage, I build robust analytical and AI tooling that makes complex work faster and more consistent.

At a large U.S. GSIB, I supported a large-scale AML remediation effort by conducting financial crime investigations and converting complex transactional activity and customer-risk signals into regulator-ready documentation. I analyzed alerts and case histories, reconstructed activity across accounts and counterparties, assessed risk factors against AML policy and risk appetite, and produced clear, defensible narratives to support escalation and filing decisions at high volume.

At a large U.S. bank, I supported a KYC remediation program by remediating customer due diligence files to bring profiles back into alignment with policy and regulatory expectations. This included refreshing customer identification and ownership/control information, validating key attributes used for risk rating, completing required screenings and adverse-information checks, documenting rationale and evidence in the bank’s systems, and escalating gaps or inconsistencies so records were complete, consistent, and review-ready.

For a Group B bank, I contributed to an FDIC IDI resolution plan by developing core finance and risk sections and translating fragmented inputs into coherent, regulator-facing narratives. I also performed a gap analysis between current practices and regulatory expectations by mapping requirements to existing policies, processes, and evidence, identifying where documentation or controls were insufficient, and coordinating with stakeholders to close gaps through targeted inputs and updated narrative support—resulting in a more internally consistent, evidence-backed submission that was structured for regulatory review and iterative challenge.

As a research assistant at a top-tier university, I build production-grade data and document pipelines in Python on distributed/HPC environments, applying modern NLP/LLM methods to turn messy, unstructured sources into analysis-ready datasets. I also serve as a statistical programmer on applied research projects, implementing causal inference and econometric workflows, running robustness checks, and validating results end-to-end with reproducible code, version control, and disciplined QA.

