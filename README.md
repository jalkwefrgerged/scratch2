* As a research assistant at a top-tier university, I built research-grade data and analysis workflows that helped applied economics projects move from messy source documents to defensible, reproducible results.

  * Programmed LLM-enabled document pipelines for economics research, spanning large-scale document retrieval, preprocessing (including OCR when needed), section-level retrieval for long documents, structured extraction into predefined schemas, and end-to-end provenance/traceability so every extracted field can be audited back to the original source text.

    * AI + Zoning: Built an end-to-end pipeline to discover historical zoning ordinances in newspaper archives (often only available as scans) and convert dense regulatory language into standardized measures of housing supply constraints, with traceability back to the underlying issue/page and supporting passages.
    * Corporate Agreements: Built a pipeline to extract pricing and covenant terms from EDGAR credit agreement exhibits by retrieving and anchoring raw exhibits, segmenting and classifying key sections, extracting into a consistent schema, and exporting structured outputs with filing/section/span references to preserve provenance for downstream empirical work.
  * Statistical programmer for applied economics research papers, producing publication-ready estimates and figures with replication-friendly code and clear documentation for coauthor review and robustness work.

    * Implemented causal designs including DiD/DDD and event-study models, with extensions for IV, heterogeneity, and robustness analysis to support credible interpretation and transparent reporting.
  * Created a coding-agent workflow on top of the Codex SDK to automate common RA tasks and shorten iteration cycles from idea to cleaned data to initial results.

    * Automated repetitive data wrangling and exploratory analysis into unattended runs, keeping outputs organized and reviewable while reducing manual overhead.
